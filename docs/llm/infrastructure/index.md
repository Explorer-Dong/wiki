---
title: 基础设施
---

目标：让读者理解如何部署、加速与运维大模型。文章偏向“如何把 LLM 跑得更快、更稳、更便宜”。

潜在文章主题：

1. 推理优化
    - 模型格式转换：从 PyTorch 到 ONNX/TensorRT 的流水线
    - 量化 vs 剪枝 vs 蒸馏：压缩大模型的三把刀
    - KV Cache：为什么能让推理提速数倍
    - Batching 策略：一个人点菜 vs 一桌人拼单
2. 部署架构
    - 单机部署：用 FastAPI 包装你的模型
    - 多机部署：分布式推理的调度思路
    - GPU 资源池化：K8s + 调度器的基本原理
    - 异构硬件：CPU、GPU、TPU、Ascend 的对比与取舍
3. 可观测性与可靠性
    - 模型服务的指标：QPS、P95 延迟、吞吐量
    - 如何监控显存、日志与异常
    - A/B 测试与灰度发布：上线模型的工程套路
4. 成本与安全
    - 云端与本地的部署对比：性能 vs 成本
    - 数据隐私与合规：为什么要有数据脱敏
    - 安全输入输出：防 prompt 注入与越狱攻击
5. 扩展文章
    - 自动化 CI/CD：持续集成大模型的新挑战
    - 大规模推理平台案例：ChatGPT、Claude 等背后的基础设施猜想
