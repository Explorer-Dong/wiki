---
title: 自然语言处理导读
icon: material/translate
---

本文记录自然语言处理的学习笔记。参考 CS 224N [主页](https://web.stanford.edu/class/cs224n/index.html) [视频](https://www.bilibili.com/video/BV1U5RNYgEfp)。

自然语言处理 (Natural Language Processing, NLP) 是人工智能研究的一个子领域，主要利用文本模态的数据来学习拟合函数。随着研究的创新与突破，NLP 的研究范式主要经历了以下四个阶段：

- 第一范式（约 2014 年以前）：传统机器学习（监督学习）。依赖人工设计特征与统计模型（如 SVM、CRF），通过标注数据完成分类或标注任务。特征工程成本高，跨任务迁移性差；
- 第二范式（2014—2017）：深度神经网络（监督学习）。以 CNN、RNN 等为代表，模型自动学习特征，提升了端到端建模能力。但仍需大量标注数据，每个任务需单独训练；
- 第三范式（2017—Present）：预训练 + 微调（自监督学习 + 监督学习）。以 BERT、GPT、T5 为代表，模型先在大规模语料 (Corpus) 上自监督学习通用语言表征，再通过少量标注数据微调到下游任务，大幅提升模型的迁移与泛化能力；
- 第四范式（2022—Present）：预训练 + 提示（自监督学习 + 直接预测）。以 GPT 为代表，大模型通过自然语言提示 (Prompt) 直接完成任务，无需额外微调，实现零样本或少样本学习，展现出了通用智能。

整体来看，NLP 的研究范式正从「任务特定」向「通用智能」转变。未来的趋势将是更强的推理能力、更丰富的世界知识，以及语言和其他模态认知的融合。

本文主要围绕第二范式展开，分别介绍 [静态词嵌入](./word-embedding.md)、[文本分类](./text-classification.md) 和 [文本生成](./text-generation.md)。第一范式不再提及，第三和第四范式涉及到较新的研究内容，感兴趣的读者可以移步 [大模型](../../../llm/index.md) 专栏作进一步阅读。
